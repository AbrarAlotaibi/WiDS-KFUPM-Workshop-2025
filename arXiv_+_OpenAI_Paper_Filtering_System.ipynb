{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrarAlotaibi/WiDS-KFUPM-Workshop-2025/blob/main/arXiv_%2B_OpenAI_Paper_Filtering_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# arXiv + OpenAI Paper Filtering System\n",
        "\n",
        "\n",
        "By: Abrar Alotaibi\n",
        "\n",
        "This notebook demonstrates how to combine the arXiv API with OpenAI's language models to create an intelligent paper filtering system for research.\n",
        "\n",
        "The system works in 3 main steps:\n",
        " 1. Query arXiv API to get papers based on keyword search\n",
        " 2. Use OpenAI to analyze each paper's relevance to your specific research interest\n",
        " 3. Filter, rank, and visualize the most relevant papers\n",
        "\n",
        "This approach is much more powerful than simple keyword filtering because it\n",
        "uses AI to understand the semantic relevance of papers to your research."
      ],
      "metadata": {
        "id": "z_t7e1hHlZZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary packages and import libraries"
      ],
      "metadata": {
        "id": "4XAVIkL0mYU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we need to install the required packages if they aren't already available\n",
        "\n",
        "# Uncomment and run these lines if you need to install any packages\n",
        "#!pip install openai requests pandas matplotlib tqdm\n",
        "#!pip install ipywidgets  # For interactive UI\n",
        "#!pip install wordcloud   # Optional: for creating word clouds of key concepts\n",
        "\n",
        "# Now import all the libraries we'll need\n",
        "import requests                      # For making HTTP requests to arXiv API\n",
        "import xml.etree.ElementTree as ET   # For parsing XML responses from arXiv\n",
        "import openai                        # For accessing OpenAI models\n",
        "import json                          # For handling JSON data\n",
        "import time                          # For adding delays between API calls\n",
        "import pandas as pd                  # For data manipulation and analysis\n",
        "from IPython.display import display, HTML, Markdown  # For rich output in Colab\n",
        "import matplotlib.pyplot as plt      # For data visualization\n",
        "from datetime import datetime        # For date formatting\n",
        "import re                            # For regular expressions\n",
        "import warnings                      # To suppress unnecessary warnings\n",
        "warnings.filterwarnings('ignore')    # Keep output clean\n",
        "\n",
        "# Import tqdm for progress bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "u-YK2gLDlcil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up your OpenAI API key"
      ],
      "metadata": {
        "id": "F2Uap1Ugm0PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell sets up your OpenAI API key, which is required to use their models.\n",
        "# You need to replace the placeholder with your actual API key.\n",
        "# go to https://platform.openai.com/settings/profile/user\n",
        "\n",
        "# Replace this with your actual OpenAI API key\n",
        "OPENAI_API_KEY = \"your-openai-api-key-here\"\n",
        "\n",
        "# Set the API key for the OpenAI library\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Verify API key is set (this doesn't check if it's valid, just that it's set)\n",
        "if openai.api_key and openai.api_key != \"your-openai-api-key-here\":\n",
        "    print(\"✅ API key configured.\")\n",
        "else:\n",
        "    print(\"⚠️ Please replace the placeholder with your actual OpenAI API key.\")"
      ],
      "metadata": {
        "id": "lOqAKGX0m5Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to query the arXiv API"
      ],
      "metadata": {
        "id": "GfU3CIrZora_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The arXiv API allows us to programmatically search for academic papers.\n",
        "# This function queries the API and parses the XML response into a list of paper objects.\n",
        "#\n",
        "# Key concepts:\n",
        "# - arXiv API query structure\n",
        "# - XML parsing with ElementTree\n",
        "# - Handling namespaces in XML\n",
        "\n",
        "def fetch_arxiv_papers(query, max_results=50):\n",
        "    \"\"\"\n",
        "    Query the arXiv API for papers based on search terms\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str\n",
        "        Search query (e.g., \"machine learning\")\n",
        "    max_results : int\n",
        "        Maximum number of results to return (default: 50)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    list\n",
        "        List of dictionaries, each containing paper information\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Format the query for arXiv API\n",
        "        # Replace spaces with '+' for URL formatting\n",
        "        formatted_query = query.replace(' ', '+')\n",
        "\n",
        "        # Step 2: Construct the API URL\n",
        "        # The arXiv API uses a REST-style interface with query parameters\n",
        "        arxiv_url = f\"http://export.arxiv.org/api/query?search_query=all:{formatted_query}&start=0&max_results={max_results}\"\n",
        "\n",
        "        print(f\"Fetching papers from arXiv with query: {query}\")\n",
        "\n",
        "        # Step 3: Make the HTTP request to the arXiv API\n",
        "        response = requests.get(arxiv_url)\n",
        "\n",
        "        # Step 4: Parse XML response using ElementTree\n",
        "        root = ET.fromstring(response.text)\n",
        "\n",
        "        # Step 5: Define namespace for XML parsing\n",
        "        # arXiv API uses the Atom XML format which requires namespace handling\n",
        "        namespace = {'atom': 'http://www.w3.org/2005/Atom'}\n",
        "\n",
        "        # Step 6: Extract all entries (papers) from the response\n",
        "        entries = root.findall('atom:entry', namespace)\n",
        "\n",
        "        if not entries:\n",
        "            print(\"No papers found matching the query\")\n",
        "            return []\n",
        "\n",
        "        # Step 7: Process each paper entry\n",
        "        papers = []\n",
        "        for entry in entries:\n",
        "            # Extract authors (there may be multiple)\n",
        "            authors = entry.findall('atom:author', namespace)\n",
        "            author_names = [author.find('atom:name', namespace).text for author in authors]\n",
        "\n",
        "            # Extract categories (subject areas)\n",
        "            categories = entry.findall('atom:category', namespace)\n",
        "            category_terms = [category.get('term') for category in categories]\n",
        "\n",
        "            # Create a dictionary for this paper with all relevant information\n",
        "            paper = {\n",
        "                'title': entry.find('atom:title', namespace).text.strip().replace('\\n', ' '),\n",
        "                'authors': author_names,\n",
        "                'summary': entry.find('atom:summary', namespace).text.strip().replace('\\n', ' '),\n",
        "                'published': entry.find('atom:published', namespace).text,\n",
        "                'updated': entry.find('atom:updated', namespace).text,\n",
        "                'link': entry.find('atom:id', namespace).text,\n",
        "                'categories': category_terms\n",
        "            }\n",
        "            papers.append(paper)\n",
        "\n",
        "        print(f\"Successfully retrieved {len(papers)} papers.\")\n",
        "        return papers\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching papers from arXiv: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Test the function (optional)\n",
        "# You can uncomment this to test just the arXiv query function\n",
        "#test_papers = fetch_arxiv_papers(\"quantum computing\", max_results=5)\n",
        "#print(f\"Example paper title: {test_papers[0]['title']}\")"
      ],
      "metadata": {
        "id": "L8IHsudPoM-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to analyze paper relevance using OpenAI"
      ],
      "metadata": {
        "id": "kCrq5OYzphd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function uses OpenAI's language models to analyze the relevance of each paper\n",
        "# to the specific research interest provided by the user.\n",
        "#\n",
        "# Key concepts:\n",
        "# - Prompt engineering for effective results\n",
        "# - Using OpenAI's chat completions API\n",
        "# - JSON response formatting\n",
        "# - Error handling for API calls\n",
        "\n",
        "def analyze_paper_relevance(paper, research_interest):\n",
        "    \"\"\"\n",
        "    Analyze paper relevance using OpenAI's language models\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    paper : dict\n",
        "        Paper object containing title, abstract, etc.\n",
        "    research_interest : str\n",
        "        User's specific research interest\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Original paper object enhanced with relevance score, explanation, and key concepts\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Construct a detailed prompt for OpenAI\n",
        "        # This prompt is crucial for getting good results - it needs to:\n",
        "        # - Provide sufficient context from the paper\n",
        "        # - Clearly state the research interest\n",
        "        # - Specify exactly what information we want in return\n",
        "        prompt = f\"\"\"\n",
        "              Paper Title: {paper['title']}\n",
        "              Paper Abstract: {paper['summary']}\n",
        "              Paper Categories: {', '.join(paper['categories'])}\n",
        "\n",
        "              Research Interest: {research_interest}\n",
        "\n",
        "              Task: Evaluate the relevance of this paper to the research interest.\n",
        "              1) Provide a relevance score from 0-100 where 0 is completely irrelevant and 100 is extremely relevant\n",
        "              2) Explain in 2-3 sentences why this paper is or isn't relevant to the research interest\n",
        "              3) Extract key concepts from the paper that match the research interest\n",
        "\n",
        "              Format your response as JSON:\n",
        "              {{\n",
        "                \"relevanceScore\": [number between 0-100],\n",
        "                \"explanation\": [explanation text],\n",
        "                \"keyMatchingConcepts\": [array of key concepts]\n",
        "              }}\n",
        "\"\"\"\n",
        "\n",
        "        # Step 2: Call the OpenAI API with our prompt\n",
        "        # We use the chat completions API with specific parameters:\n",
        "        # - model: Which model to use (gpt-4o-mini is powerful but you could use others)\n",
        "        # - system message: Sets the context and role for the AI\n",
        "        # - user message: Our detailed prompt\n",
        "        # - temperature: Lower values make output more deterministic/consistent\n",
        "        # - response_format: Request JSON formatting for easier parsing\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",  # You can change this to other models like \"gpt-3.5-turbo\" if needed\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a research assistant specialized in analyzing scientific papers and determining their relevance to specific research interests. Be precise in your evaluations.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,  # Low temperature for more consistent outputs\n",
        "            response_format={\"type\": \"json_object\"}  # Request JSON formatted response\n",
        "        )\n",
        "\n",
        "        # Step 3: Parse the response\n",
        "        analysis_text = response.choices[0].message.content\n",
        "        analysis = json.loads(analysis_text)\n",
        "\n",
        "        # Step 4: Add the analysis results to the paper object\n",
        "        paper['relevanceScore'] = analysis['relevanceScore']\n",
        "        paper['relevanceExplanation'] = analysis['explanation']\n",
        "        paper['keyMatchingConcepts'] = analysis['keyMatchingConcepts']\n",
        "\n",
        "        return paper\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing paper: {paper['title']}\")\n",
        "        print(f\"Error details: {e}\")\n",
        "\n",
        "        # If an error occurs, return the paper with default values\n",
        "        paper['relevanceScore'] = 0\n",
        "        paper['relevanceExplanation'] = \"Error occurred during analysis\"\n",
        "        paper['keyMatchingConcepts'] = []\n",
        "        return paper\n",
        "\n",
        "# Test the function (optional)\n",
        "# You can uncomment this to test just the OpenAI analysis function\n",
        "# Note: This requires both the fetch_arxiv_papers function and a valid API key\n",
        "'''\n",
        "test_paper = fetch_arxiv_papers(\"reinforcement learning\", max_results=1)[0]\n",
        "test_interest = \"Applications of reinforcement learning in robotic control\"\n",
        "analyzed_paper = analyze_paper_relevance(test_paper, test_interest)\n",
        "print(f\"Relevance score: {analyzed_paper['relevanceScore']}\")\n",
        "print(f\"Explanation: {analyzed_paper['relevanceExplanation']}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "QqoCSauNo93e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function to search, analyze, and filter papers"
      ],
      "metadata": {
        "id": "wpnsyrA5reyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function combines our previous functions to:\n",
        "# 1. Search arXiv for papers\n",
        "# 2. Analyze each paper with OpenAI\n",
        "# 3. Filter papers based on relevance\n",
        "# 4. Sort the results\n",
        "#\n",
        "# Key concepts:\n",
        "# - Batch processing for API efficiency\n",
        "# - Progress tracking with tqdm\n",
        "# - Rate limiting to avoid API throttling\n",
        "# - Sorting and filtering data\n",
        "\n",
        "def find_relevant_papers(initial_query, research_interest, relevance_threshold=50):\n",
        "    \"\"\"\n",
        "    Main function to search and filter papers\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    initial_query : str\n",
        "        Initial arXiv search query (broader terms)\n",
        "    research_interest : str\n",
        "        Detailed research interest for filtering (more specific)\n",
        "    relevance_threshold : int\n",
        "        Minimum relevance score (0-100) for papers to be included\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    list\n",
        "        Filtered and sorted papers\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step a: Initial search - fetch papers from arXiv\n",
        "        # This uses our previous function to get a list of papers\n",
        "        papers = fetch_arxiv_papers(initial_query)\n",
        "        print(f\"Found {len(papers)} papers from initial search.\")\n",
        "\n",
        "        if len(papers) == 0:\n",
        "            return []\n",
        "\n",
        "        # Step b: Process papers with OpenAI analysis\n",
        "        # We do this in batches with a progress bar to:\n",
        "        # - Give visual feedback on progress\n",
        "        # - Avoid overwhelming the API\n",
        "        # - Allow for graceful error handling\n",
        "        batch_size = 5  # Processing 5 papers at a time\n",
        "        analyzed_papers = []\n",
        "\n",
        "        # Create a progress bar\n",
        "        print(\"Analyzing papers with OpenAI (this may take a few minutes)...\")\n",
        "        for i in tqdm(range(0, len(papers), batch_size)):\n",
        "            # Get the current batch of papers\n",
        "            batch = papers[i:i + batch_size]\n",
        "\n",
        "            # Process each paper in the batch\n",
        "            for paper in batch:\n",
        "                analyzed_paper = analyze_paper_relevance(paper, research_interest)\n",
        "                analyzed_papers.append(analyzed_paper)\n",
        "                # Add a small delay to avoid rate limits\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        # Step c: Filter papers based on relevance threshold\n",
        "        # Only keep papers that scored above our threshold\n",
        "        filtered_papers = [paper for paper in analyzed_papers if paper['relevanceScore'] >= relevance_threshold]\n",
        "\n",
        "        # Step d: Sort papers by relevance score (highest first)\n",
        "        sorted_papers = sorted(filtered_papers, key=lambda x: x['relevanceScore'], reverse=True)\n",
        "\n",
        "        print(f\"Filtering complete. Found {len(sorted_papers)} papers with relevance score ≥ {relevance_threshold}.\")\n",
        "        return sorted_papers\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in find_relevant_papers: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Test the full search pipeline (optional)\n",
        "# You can uncomment this to test the full pipeline\n",
        "# Note: This will use OpenAI API credits\n",
        "\n",
        "results = find_relevant_papers(\n",
        "    initial_query=\"reinforcement learning robotics\",\n",
        "    research_interest=\"Applications of reinforcement learning in robotic manipulation with sparse rewards\",\n",
        "    relevance_threshold=70\n",
        ")\n",
        "print(f\"Found {len(results)} highly relevant papers\")\n"
      ],
      "metadata": {
        "id": "aoU3v3jRq3He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to visualize results"
      ],
      "metadata": {
        "id": "6UbxtZBnsKg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These functions create visualizations and formatted displays of our results.\n",
        "# Good visualizations are essential for interpreting the results of our analysis.\n",
        "#\n",
        "# Key concepts:\n",
        "# - Data visualization with matplotlib\n",
        "# - Formatted HTML output in notebooks\n",
        "# - Optional word cloud for concept visualization\n",
        "# - DataFrame creation for further analysis\n",
        "\n",
        "# First function: Create data visualizations\n",
        "def visualize_results(papers):\n",
        "    \"\"\"\n",
        "    Create visualizations of the paper results\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    papers : list\n",
        "        List of paper objects with relevance scores\n",
        "    \"\"\"\n",
        "    if not papers:\n",
        "        display(Markdown(\"## No relevant papers found.\"))\n",
        "        return\n",
        "\n",
        "    # Create a DataFrame for easier data manipulation\n",
        "    df = pd.DataFrame(papers)\n",
        "\n",
        "    # Visualization 1: Histogram of relevance scores\n",
        "    # Shows the distribution of relevance scores across all papers\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(df['relevanceScore'], bins=10, alpha=0.7, color='skyblue')\n",
        "    plt.title('Distribution of Relevance Scores')\n",
        "    plt.xlabel('Relevance Score')\n",
        "    plt.ylabel('Number of Papers')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Visualization 2: Top papers bar chart\n",
        "    # Shows the most relevant papers (up to 10) sorted by score\n",
        "    top_papers = df.sort_values('relevanceScore', ascending=False).head(10)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Truncate long titles for better display\n",
        "    truncated_titles = top_papers['title'].str[:50] + '...'\n",
        "\n",
        "    bars = plt.barh(truncated_titles, top_papers['relevanceScore'], color='skyblue')\n",
        "    plt.xlabel('Relevance Score')\n",
        "    plt.title('Top 10 Papers by Relevance')\n",
        "    plt.tight_layout()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Visualization 3 (Optional): Word cloud of key concepts\n",
        "    # Creates a word cloud from all the key concepts across papers\n",
        "    try:\n",
        "        from wordcloud import WordCloud\n",
        "\n",
        "        # Flatten all key concepts into a single list\n",
        "        all_concepts = []\n",
        "        for concepts in df['keyMatchingConcepts']:\n",
        "            all_concepts.extend(concepts)\n",
        "\n",
        "        # Create a string of all concepts (word cloud needs text input)\n",
        "        text = ' '.join(all_concepts)\n",
        "\n",
        "        # Generate word cloud\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
        "                              max_words=100, contour_width=3, contour_color='steelblue')\n",
        "        wordcloud.generate(text)\n",
        "\n",
        "        # Display the word cloud\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title('Key Concepts Word Cloud')\n",
        "        plt.show()\n",
        "    except ImportError:\n",
        "        print(\"WordCloud not installed. Run: !pip install wordcloud\")\n",
        "\n",
        "# Second function: Display formatted results as an HTML table\n",
        "def display_results(papers):\n",
        "    \"\"\"\n",
        "    Format and display the results in a nice HTML table\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    papers : list\n",
        "        List of paper objects\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame\n",
        "        Pandas DataFrame containing the paper data\n",
        "    \"\"\"\n",
        "    if not papers:\n",
        "        display(Markdown(\"## No relevant papers found.\"))\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    display(Markdown(f\"## Found {len(papers)} relevant papers\"))\n",
        "\n",
        "    # Create an HTML table with styling for better display in Colab\n",
        "    html = \"\"\"\n",
        "    <style>\n",
        "        .paper-table {width: 100%; border-collapse: collapse; margin-bottom: 20px;}\n",
        "        .paper-table th {background-color: #4CAF50; color: white; text-align: left; padding: 12px;}\n",
        "        .paper-table td {padding: 12px; border-bottom: 1px solid #ddd;}\n",
        "        .paper-title {font-weight: bold; font-size: 1.1em;}\n",
        "        .relevance-high {color: green; font-weight: bold;}\n",
        "        .relevance-medium {color: orange; font-weight: bold;}\n",
        "        .relevance-low {color: red; font-weight: bold;}\n",
        "        .authors {font-style: italic;}\n",
        "        .paper-row:hover {background-color: #f5f5f5;}\n",
        "    </style>\n",
        "    <table class=\"paper-table\">\n",
        "        <tr>\n",
        "            <th>Title & Relevance</th>\n",
        "            <th>Details</th>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "\n",
        "    for i, paper in enumerate(papers):\n",
        "        # Determine relevance class for color-coding\n",
        "        if paper['relevanceScore'] >= 80:\n",
        "            relevance_class = \"relevance-high\"  # Green for highly relevant\n",
        "        elif paper['relevanceScore'] >= 60:\n",
        "            relevance_class = \"relevance-medium\"  # Orange for moderately relevant\n",
        "        else:\n",
        "            relevance_class = \"relevance-low\"  # Red for marginally relevant\n",
        "\n",
        "        # Format the publication date\n",
        "        pub_date = datetime.strptime(paper['published'], '%Y-%m-%dT%H:%M:%SZ').strftime('%b %d, %Y')\n",
        "\n",
        "        # Create HTML row for this paper\n",
        "        html += f\"\"\"\n",
        "        <tr class=\"paper-row\">\n",
        "            <td>\n",
        "                <div class=\"paper-title\">{i+1}. {paper['title']}</div>\n",
        "                <div class=\"{relevance_class}\">Relevance: {paper['relevanceScore']}%</div>\n",
        "            </td>\n",
        "            <td>\n",
        "                <div class=\"authors\">Authors: {', '.join(paper['authors'])}</div>\n",
        "                <div>Published: {pub_date}</div>\n",
        "                <div>Link: <a href=\"{paper['link']}\" target=\"_blank\">{paper['link']}</a></div>\n",
        "                <div><strong>Relevance Explanation:</strong> {paper['relevanceExplanation']}</div>\n",
        "                <div><strong>Key Concepts:</strong> {', '.join(paper['keyMatchingConcepts'])}</div>\n",
        "            </td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    html += \"</table>\"\n",
        "\n",
        "    # Display the HTML table in the notebook\n",
        "    display(HTML(html))\n",
        "\n",
        "    # Also create a DataFrame for further analysis or export\n",
        "    df = pd.DataFrame(papers)\n",
        "    return df"
      ],
      "metadata": {
        "id": "ESjHBXeksGTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive UI for paper search"
      ],
      "metadata": {
        "id": "399MI3cEs1ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function creates an interactive user interface using ipywidgets.\n",
        "# It's perfect for teaching because students can modify parameters and see results immediately.\n",
        "#\n",
        "# Key concepts:\n",
        "# - Interactive widgets in Jupyter/Colab\n",
        "# - Event handling for button clicks\n",
        "# - Dynamic output updates\n",
        "# - Data export\n",
        "\n",
        "def run_interactive_search():\n",
        "    \"\"\"\n",
        "    Create an interactive UI for paper search using ipywidgets\n",
        "    \"\"\"\n",
        "    # Import widgets library for interactive UI components\n",
        "    from ipywidgets import widgets\n",
        "    from IPython.display import display\n",
        "\n",
        "    # Create input widgets\n",
        "    # Text widget for the initial arXiv query\n",
        "    query_widget = widgets.Text(\n",
        "        value='machine learning reinforcement learning',  # Default value\n",
        "        description='Initial Query:',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout={'width': '50%'},\n",
        "        tooltip='Enter keywords for the initial arXiv search (broader terms)'\n",
        "    )\n",
        "\n",
        "    # Textarea for more detailed research interest\n",
        "    interest_widget = widgets.Textarea(\n",
        "        value='Applications of reinforcement learning in robotic control systems with sparse rewards',  # Default value\n",
        "        description='Research Interest:',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout={'width': '70%', 'height': '80px'},\n",
        "        tooltip='Enter your specific research interest in detail (more specific)'\n",
        "    )\n",
        "\n",
        "    # Slider for relevance threshold\n",
        "    threshold_widget = widgets.IntSlider(\n",
        "        value=70,  # Default value\n",
        "        min=0,\n",
        "        max=100,\n",
        "        step=5,\n",
        "        description='Relevance Threshold:',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout={'width': '50%'},\n",
        "        tooltip='Set minimum relevance score (0-100) for including papers'\n",
        "    )\n",
        "\n",
        "    # Create an output widget to display results\n",
        "    results_widget = widgets.Output()\n",
        "\n",
        "    # Create search button\n",
        "    button = widgets.Button(\n",
        "        description='Search Papers',\n",
        "        button_style='success',  # Green button\n",
        "        icon='search',\n",
        "        tooltip='Click to start the search process'\n",
        "    )\n",
        "\n",
        "    # Define button click event handler\n",
        "    def on_button_click(b):\n",
        "        with results_widget:\n",
        "            # Clear previous output\n",
        "            results_widget.clear_output()\n",
        "            print(\"Searching for papers...\")\n",
        "            try:\n",
        "                # Run the search with current widget values\n",
        "                papers = find_relevant_papers(\n",
        "                    query_widget.value,\n",
        "                    interest_widget.value,\n",
        "                    threshold_widget.value\n",
        "                )\n",
        "\n",
        "                # Display results\n",
        "                df = display_results(papers)\n",
        "                visualize_results(papers)\n",
        "\n",
        "                # Save results to CSV if we found any papers\n",
        "                if len(papers) > 0:\n",
        "                    df.to_csv('arxiv_filtered_papers.csv')\n",
        "                    print(\"Results saved to 'arxiv_filtered_papers.csv'\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during search: {e}\")\n",
        "\n",
        "    # Connect the button click event to our handler function\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "    # Display all the widgets\n",
        "    display(widgets.HTML(\"<h2>arXiv + OpenAI Paper Filtering</h2>\"))\n",
        "    display(widgets.HTML(\"<p>Enter a broad initial query for arXiv, then specify your detailed research interest for AI filtering.</p>\"))\n",
        "    display(query_widget)\n",
        "    display(interest_widget)\n",
        "    display(threshold_widget)\n",
        "    display(button)\n",
        "    display(results_widget)\n",
        "\n",
        "# This function creates an interactive UI for students to experiment with\n",
        "# Uncomment the line below to run the interactive UI\n",
        "# run_interactive_search()"
      ],
      "metadata": {
        "id": "VSWc1XUasv74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple example usage"
      ],
      "metadata": {
        "id": "H_rSxZqrtZDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This cell shows a simple example of using our system with predefined parameters.\n",
        "# It's useful for demonstrating the complete pipeline without interactive widgets.\n",
        "#\n",
        "# this lets you:\n",
        "# - Run a predefined example\n",
        "# - Show the complete workflow in one step\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run the paper search and analysis with predefined parameters\n",
        "    \"\"\"\n",
        "    # Example search parameters - feel free to modify these for your demonstration\n",
        "    initial_query = \"machine learning reinforcement learning\"\n",
        "    research_interest = \"Applications of reinforcement learning in robotic control systems with sparse rewards\"\n",
        "    relevance_threshold = 70\n",
        "\n",
        "    print(f\"Searching for papers related to: {research_interest}\")\n",
        "    print(f\"Initial arXiv query: {initial_query}\")\n",
        "    print(f\"Relevance threshold: {relevance_threshold}%\\n\")\n",
        "\n",
        "    # Run the search and analysis pipeline\n",
        "    papers = find_relevant_papers(initial_query, research_interest, relevance_threshold)\n",
        "\n",
        "    # Display and visualize the results\n",
        "    df = display_results(papers)\n",
        "    visualize_results(papers)\n",
        "\n",
        "    # Save results to CSV\n",
        "    if len(papers) > 0:\n",
        "        df.to_csv('arxiv_filtered_papers.csv')\n",
        "        print(\"Results saved to 'arxiv_filtered_papers.csv'\")\n",
        "\n",
        "    return papers  # Return the papers for further analysis if needed\n",
        "\n",
        "# Uncomment one of the following lines to run either:\n",
        "# The simple example with predefined parameters:\n",
        "# papers = main()\n",
        "\n",
        "# Or the interactive version:\n",
        "#run_interactive_search()"
      ],
      "metadata": {
        "id": "uVh8qx2Cs_es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_x3Utqx5tr77"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}